# Bias, Data privacy, Explainability, and the black box problem in modern AI systems

This project was a group presentation (Group 6) exploring the critical ethical challenges at the intersection of Machine Learning and Deep Learning. As AI systems become more powerful and integrated into our daily lives, it is essential to address the complex issues they raise to ensure they are used responsibly.

## Objective

The primary objective of this project was to research, discuss, and present a clear overview of the key ethical hurdles in modern AI. The goal was to move beyond the technical implementation and focus on the societal impact, balancing the potential for innovation with the need for ethical responsibility.

## Key Ethical Challenges Explored

Our research focused on four interconnected pillars of AI ethics:

### 1. Bias and Fairness

AI learns from data, and if that data reflects historical inequalities or societal biases, the AI will learn and amplify those same biases.

* **Real-World Impact:** We examined discrimination in AI-driven hiring (Amazon's recruiting tool), credit scoring (Apple Card), and facial recognition.
* **Solutions:** We discussed the importance of using diverse, representative datasets, conducting fairness audits, and maintaining human oversight.

### 2. Data Privacy and Security

This section covered the challenges of protecting sensitive personal information in an age of massive data collection.

* **Key Issues:** We explored the risks of data leakage, the difficulty of ensuring user consent at scale, and the potential for misuse of training data.
* **The Privacy Paradox:** We also discussed the societal question of why we accept data use by some services (like Spotify) but resist it from others (like insurers).

### 3. The 'Black Box' Problem

Many advanced models, especially deep learning neural networks, are "opaque." They can have billions of parameters, making it nearly impossible for a human to understand *how* or *why* they arrived at a specific decision.

### 4. Explainability (XAI)

As a solution to the "black box" problem, we presented the field of "Explainable AI" (XAI).

* **Real-World Consequences:** Without explainability, we cannot fully trust or debug AI. If an AI denies a mortgage or misdiagnoses a patient, we have a right to know why.
* **The Path Forward:** XAI techniques (like LIME and SHAP) are being developed to make AI models more transparent, accountable, and trustworthy.

## Tools and Technologies Used

* **Research:** Used LLMs (ChatGPT, Google Gemini) and academic/news sources (Reuters, The Verge, ACM, Nature) to gather information and case studies.
* **Presentation:** Microsoft PowerPoint was used to structure, visualize, and present our findings.
* **Collaboration:** This was a group project requiring coordination, discussion, and synthesis of research from all members (Akhil, Riyaz, Deepika, Muhammad Gufran, Karthik Ram).

---
[Download the original presentation (PDF)](./assets/p2.pdf)

---

* [Back to Projects](./projects)
* [Back to Home](./)
